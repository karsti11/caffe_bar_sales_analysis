{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7849560-4dc9-41a7-acfc-709b8b806ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "rootPath = os.path.split(sys.path[0])[0]\n",
    "sys.path.append(rootPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03ccb38e-9467-4565-b12f-dce72c69d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(rootPath, filename):\n",
    "    data_df = pd.read_csv(os.path.join(rootPath, filename))\n",
    "    data_df.sales_datetime = pd.to_datetime(data_df.sales_datetime, format='%Y-%m-%d', utc=True)\n",
    "    data_df.set_index('sales_datetime', inplace=True)\n",
    "    return data_df\n",
    "\n",
    "def visualize_seasonality(item_daily_data_df):\n",
    "    year_month_mean = item_daily_data_df.groupby(pd.Grouper(freq='MS')).agg({'sales_qty':'mean'})\n",
    "    month_mean = item_daily_data_df.groupby(item_daily_data_df.index.month).agg({'sales_qty':'mean'})\n",
    "    day_of_week_mean = item_daily_data_df.groupby(item_daily_data_df.index.day_of_week).agg({'sales_qty':'mean'})\n",
    "    day_of_month_mean = item_daily_data_df.groupby(item_daily_data_df.index.day).agg({'sales_qty':'mean'})\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20,10))\n",
    "    ax1.plot(year_month_mean)\n",
    "    ax1.set_title(\"Overall trend (averaged monthly)\")\n",
    "    ax2.plot(month_mean)\n",
    "    ax2.set_title(\"Monthly seasonality\")\n",
    "    ax3.plot(day_of_week_mean)\n",
    "    ax3.set_title(\"Day of week average pattern\")\n",
    "    ax4.plot(day_of_month_mean)\n",
    "    ax4.set_title(\"Day of month average pattern\")\n",
    "\n",
    "def fill_time_series(raw_data_df):\n",
    "    # Dataframe must have DatetimeIndex format='%Y-%m-%d'\n",
    "    data_df = raw_data_df.resample('D').sum()\n",
    "    data_df.item_price = data_df.item_price.ffill().bfill()\n",
    "    return data_df\n",
    "    \n",
    "    \n",
    "def add_features_to_raw_data(raw_data_df):\n",
    "    raw_data_df = raw_data_df.copy()\n",
    "    raw_data_df.loc[:,'day_of_week'] = raw_data_df.index.day_of_week\n",
    "    raw_data_df.loc[:,'month_of_year'] = raw_data_df.index.month\n",
    "    day_of_week_dummies = pd.get_dummies(raw_data_df.day_of_week, prefix='day_of_week')\n",
    "    month_of_year_dummies = pd.get_dummies(raw_data_df.month_of_year, prefix='month_of_year')\n",
    "    raw_data_df = raw_data_df.merge(day_of_week_dummies, how='left', left_index=True, right_index=True)\n",
    "    raw_data_df = raw_data_df.merge(month_of_year_dummies, how='left', left_index=True, right_index=True)\n",
    "    raw_data_df.loc[:,'year'] = raw_data_df.index.year - raw_data_df.index.year.min()\n",
    "    raw_data_df.loc[:,'first_third_of_month'] = (raw_data_df.index.day <= 10).astype('int8')\n",
    "    raw_data_df.loc[:,'second_third_of_month'] = ((raw_data_df.index.day > 10) & (raw_data_df.index.day <= 20)).astype('int8')\n",
    "    raw_data_df.loc[:,'last_third_of_month'] = (raw_data_df.index.day > 20).astype('int8')\n",
    "    raw_data_df.drop(columns=['day_of_week', 'month_of_year'], inplace=True)\n",
    "    return raw_data_df\n",
    "\n",
    "def split_dataset(all_data_df: pd.DataFrame, \n",
    "                  validation_split_date: str, \n",
    "                  independent_vars: list, \n",
    "                  dependent_var: str):\n",
    "    X_train = all_data_df[all_data_df.index < pd.to_datetime(validation_split_date, utc=True)][independent_vars].copy()\n",
    "    X_test = all_data_df[all_data_df.index >= pd.to_datetime(validation_split_date, utc=True)][independent_vars].copy()\n",
    "    y_train = all_data_df[all_data_df.index < pd.to_datetime(validation_split_date, utc=True)][dependent_var].copy()\n",
    "    y_test = all_data_df[all_data_df.index >= pd.to_datetime(validation_split_date, utc=True)][dependent_var].copy()\n",
    "    print(f\"Train dataset is from {X_train.index.min().strftime('%Y-%m-%d')} to {X_train.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Test dataset is from {X_test.index.min().strftime('%Y-%m-%d')} to {X_test.index.max().strftime('%Y-%m-%d')}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def wmape(actual: pd.Series, predicted: pd.Series):\n",
    "    score = (np.abs(actual - predicted).sum() / np.abs(actual).sum())*100\n",
    "    return round(score, 2)\n",
    "\n",
    "def smape(A, F):\n",
    "    score = 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "    return round(score, 2)\n",
    "\n",
    "def plot_fit_and_residuals(train_sales, \n",
    "                           train_predictions, \n",
    "                           val_sales, \n",
    "                           val_predictions, \n",
    "                           features_coefs_df,\n",
    "                           train_residuals, \n",
    "                           test_residuals):\n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(30,20))\n",
    "    fig.suptitle('Predictions and coefficients plot')\n",
    "    train_sales.plot(kind='line', ax=ax1)\n",
    "    train_predictions.plot(kind='line', ax=ax1)\n",
    "    \n",
    "    val_sales.plot(kind='line', ax=ax2)\n",
    "    val_predictions.plot(kind='line', ax=ax2)\n",
    "    \n",
    "    ax3.scatter(train_residuals, train_predictions)\n",
    "    ax4.scatter(test_residuals, val_predictions)\n",
    "    \n",
    "    features_coefs_df.plot(kind='barh', ax=ax5)\n",
    "    plt.show()\n",
    "    \n",
    "def time_series_cv(raw_data_filled_df, num_train_years):\n",
    "    groups = raw_data_filled_df.reset_index().groupby(raw_data_filled_df.index.year).groups\n",
    "    sorted_groups = [value.tolist() for (key, value) in sorted(groups.items())]#list of indices per year\n",
    "    if len(groups.keys()) < 2:\n",
    "        raise ValueError(\"Not enough groups for validation set.\")\n",
    "    elif len(groups.keys()) <= num_train_years+1:\n",
    "        return [(list(itertools.chain(*sorted_groups[:-1])), sorted_groups[-1])]\n",
    "    else:\n",
    "        return [(list(itertools.chain(*sorted_groups[i:num_train_years+i])), sorted_groups[i+num_train_years])\n",
    "          for i in range(len(sorted_groups) - num_train_years)]\n",
    "\n",
    "def transform_and_fit(\n",
    "    raw_data_df: pd.DataFrame, \n",
    "    independent_vars: list, \n",
    "    dependent_var: str, \n",
    "    validation_split_date: str,\n",
    "    visualize: bool):\n",
    "    \"\"\" \n",
    "    Fills missing time series data in between raw_data_df.index.min() and raw_data_df.index.max(),\n",
    "    adds calendar features, splits training dataset into training and validation dataset,\n",
    "    fits estimator on training data and predicts on validation data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    raw_data_df: Time series dataframe\n",
    "    independent_vars: list of column names to be used as independent variables for scikit-learn estimator\n",
    "    dependent_var: column name to be used as dependent variable (target) for scikit-learn estimator\n",
    "    validation_split_date: first date of validation set\n",
    "    visualize: to plot or not\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    metrics_dict: metadata and metrics for individual item\n",
    "    \n",
    "    \"\"\"\n",
    "    item_name = raw_data_df.item_name.unique()[0]\n",
    "    print(f\"Transform and fit for {item_name}\")\n",
    "    data_filled = fill_time_series(raw_data_df.copy())\n",
    "    data_w_feats = add_features_to_raw_data(data_filled)\n",
    "    X_train, X_val, y_train, y_val = split_dataset(data_w_feats, validation_split_date, independent_vars, dependent_var)\n",
    "    lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred_train = lin_reg.predict(X_train)\n",
    "    y_pred_train = pd.Series(y_pred_train, index=y_train.index)\n",
    "    train_wmape = wmape(y_train, y_pred_train)\n",
    "    #train_smape = smape(y_train, y_pred_train)\n",
    "    print(f\"Train WMAPE: {train_wmape}\")\n",
    "    #print(f\"Train SMAPE: {train_smape}\")\n",
    "    y_pred_val = lin_reg.predict(X_val)\n",
    "    y_pred_val = pd.Series(y_pred_val, index=y_val.index)\n",
    "    val_wmape = wmape(y_val, y_pred_val)\n",
    "    #val_smape = smape(y_val, y_pred_val)\n",
    "    print(f\"Validation WMAPE: {val_wmape}\")\n",
    "    #print(f\"Validation SMAPE: {val_smape}\")\n",
    "    \n",
    "    features_coefs = {var:lin_reg.coef_[idx] for idx,var in enumerate(independent_vars)}\n",
    "    features_coefs_df = pd.Series(features_coefs)\n",
    "    train_residuals = y_train - y_pred_train\n",
    "    test_residuals = y_val - y_pred_val\n",
    "    \n",
    "    if visualize:\n",
    "        plot_fit_and_residuals(y_train, \n",
    "                               y_pred_train, \n",
    "                               y_val, \n",
    "                               y_pred_val, \n",
    "                               features_coefs_df,\n",
    "                               train_residuals, \n",
    "                               test_residuals)\n",
    "    return {\n",
    "        \"item_name\": item_name,\n",
    "        \"estimator\": lin_reg.__class__.__name__,\n",
    "        \"train_from\": X_train.index.min().strftime('%Y-%m-%d'),\n",
    "        \"train_to\": X_train.index.max().strftime('%Y-%m-%d'),\n",
    "        \"val_from\": X_val.index.min().strftime('%Y-%m-%d'),\n",
    "        \"val_to\": X_val.index.max().strftime('%Y-%m-%d'),\n",
    "        \"train_wmape\": train_wmape,\n",
    "        #\"train_smape\": train_smape,\n",
    "        \"val_wmape\": val_wmape,\n",
    "        #\"val_smape\": val_smape,\n",
    "        \"features_coefs\": features_coefs\n",
    "    }\n",
    "    \n",
    "def transform_and_fit_gridsearch(\n",
    "    raw_data_df: pd.DataFrame, \n",
    "    independent_vars: list, \n",
    "    dependent_var: str, \n",
    "    validation_split_date: str,\n",
    "    visualize: bool):\n",
    "    \"\"\" \n",
    "    Fills missing time series data in between raw_data_df.index.min() and raw_data_df.index.max(),\n",
    "    adds calendar features, splits training dataset into training and validation dataset,\n",
    "    fits estimator on training data and predicts on validation data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    raw_data_df: Time series dataframe\n",
    "    independent_vars: list of column names to be used as independent variables for scikit-learn estimator\n",
    "    dependent_var: column name to be used as dependent variable (target) for scikit-learn estimator\n",
    "    validation_split_date: first date of validation set\n",
    "    visualize: to plot or not\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    metrics_dict: metadata and metrics for individual item\n",
    "    \n",
    "    \"\"\"\n",
    "    item_name = raw_data_df.item_name.unique()[0]\n",
    "    print(f\"Transform and gridsearch fit for {item_name}\")\n",
    "    data_filled = fill_time_series(raw_data_df.copy())\n",
    "    data_w_feats = add_features_to_raw_data(data_filled)\n",
    "    X_train, X_val, y_train, y_val = split_dataset(data_w_feats, validation_split_date, independent_vars, dependent_var)\n",
    "    cv_split_idxs = time_series_cv(X_train, num_train_years=3)\n",
    "    wmape_scorer = make_scorer(wmape, greater_is_better=False)\n",
    "    grid_search_params = {'alpha': [0.01, 0.1, 1.0, 10, 100]}\n",
    "    lin_reg_gscv = GridSearchCV(\n",
    "        Ridge(), \n",
    "        grid_search_params,\n",
    "        scoring=wmape_scorer,\n",
    "        cv=cv_split_idxs).fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = lin_reg_gscv.predict(X_train)\n",
    "    y_pred_train = pd.Series(y_pred_train, index=y_train.index)\n",
    "    train_wmape = wmape(y_train, y_pred_train)\n",
    "    #train_smape = smape(y_train, y_pred_train)\n",
    "    print(f\"Train WMAPE: {train_wmape}\")\n",
    "    #print(f\"Train SMAPE: {train_smape}\")\n",
    "    y_pred_val = lin_reg_gscv.predict(X_val)\n",
    "    y_pred_val = pd.Series(y_pred_val, index=y_val.index)\n",
    "    val_wmape = wmape(y_val, y_pred_val)\n",
    "    #val_smape = smape(y_val, y_pred_val)\n",
    "    print(f\"Validation WMAPE: {val_wmape}\")\n",
    "    #print(f\"Validation SMAPE: {val_smape}\")\n",
    "    \n",
    "    features_coefs = {var:lin_reg_gscv.best_estimator_.coef_[idx] for idx,var in enumerate(independent_vars)}\n",
    "    features_coefs_df = pd.Series(features_coefs)\n",
    "    train_residuals = y_train - y_pred_train\n",
    "    test_residuals = y_val - y_pred_val\n",
    "    \n",
    "    if visualize:\n",
    "        plot_fit_and_residuals(y_train, \n",
    "                               y_pred_train, \n",
    "                               y_val, \n",
    "                               y_pred_val, \n",
    "                               features_coefs_df,\n",
    "                               train_residuals, \n",
    "                               test_residuals)\n",
    "    return {\n",
    "        \"item_name\": item_name,\n",
    "        \"estimator\": lin_reg_gscv.best_estimator_.__class__.__name__,\n",
    "        \"train_from\": X_train.index.min().strftime('%Y-%m-%d'),\n",
    "        \"train_to\": X_train.index.max().strftime('%Y-%m-%d'),\n",
    "        \"val_from\": X_val.index.min().strftime('%Y-%m-%d'),\n",
    "        \"val_to\": X_val.index.max().strftime('%Y-%m-%d'),\n",
    "        \"train_wmape\": train_wmape,\n",
    "        \"val_wmape\": val_wmape,\n",
    "        \"features_coefs\": features_coefs\n",
    "    }    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "badceefc-b7c0-43fa-a6ee-a128320ac56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILENAME = 'data/interim/train_data_90_perc_value_v1_3.csv'\n",
    "train_data = load_data(rootPath, TRAIN_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035c8b0-22c4-4767-b78e-207fd5137f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby('item_name').agg({'item_name': 'count'}).rename(columns={'item_name':'num_days_sales'}).sort_values(by='num_days_sales', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb8dae-aa53-46fd-ada7-c0992bbc8f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2ed8380-7da8-4043-bbad-cf99370526c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>item_price</th>\n",
       "      <th>sales_qty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-06-14 00:00:00+00:00</th>\n",
       "      <td>Beck`s</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-15 00:00:00+00:00</th>\n",
       "      <td>Beck`s</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-16 00:00:00+00:00</th>\n",
       "      <td>Beck`s</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-17 00:00:00+00:00</th>\n",
       "      <td>Beck`s</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-18 00:00:00+00:00</th>\n",
       "      <td>Beck`s</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-09 00:00:00+00:00</th>\n",
       "      <td>Zlatni_Pan</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-10 00:00:00+00:00</th>\n",
       "      <td>Zlatni_Pan</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-12 00:00:00+00:00</th>\n",
       "      <td>Zlatni_Pan</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-14 00:00:00+00:00</th>\n",
       "      <td>Zlatni_Pan</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-16 00:00:00+00:00</th>\n",
       "      <td>Zlatni_Pan</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50063 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            item_name  item_price  sales_qty\n",
       "sales_datetime                                              \n",
       "2016-06-14 00:00:00+00:00      Beck`s        16.0        1.0\n",
       "2016-06-15 00:00:00+00:00      Beck`s        16.0        3.0\n",
       "2016-06-16 00:00:00+00:00      Beck`s        16.0        2.0\n",
       "2016-06-17 00:00:00+00:00      Beck`s        16.0        3.0\n",
       "2016-06-18 00:00:00+00:00      Beck`s        16.0        1.0\n",
       "...                               ...         ...        ...\n",
       "2016-06-09 00:00:00+00:00  Zlatni_Pan        12.0        3.0\n",
       "2016-06-10 00:00:00+00:00  Zlatni_Pan        12.0        6.0\n",
       "2016-06-12 00:00:00+00:00  Zlatni_Pan        12.0       12.0\n",
       "2016-06-14 00:00:00+00:00  Zlatni_Pan        12.0        1.0\n",
       "2016-06-16 00:00:00+00:00  Zlatni_Pan        12.0        9.0\n",
       "\n",
       "[50063 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5831a5e-d122-4348-8c3a-2d96167fe436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML_Python\\\\Programi\\\\caffe_bar_sales_analysis\\\\caffe_bar_sales_analysis\\\\notebooks'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f5d7c3c-3fd9-48e0-ae2a-e0f5f2b7eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curPath = os.path.abspath(sys.path[0])\n",
    "rootPath = os.path.split(curPath)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e49152b-dd6e-4fe6-9de2-db70dc27ea57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML_Python\\\\Programi\\\\caffe_bar_sales_analysis\\\\caffe_bar_sales_analysis'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(sys.path[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fb03a-684d-4c5d-9cf4-b355409eea76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
